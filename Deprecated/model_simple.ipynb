{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_simple.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8rDHr575m3m",
        "colab_type": "code",
        "outputId": "80525f52-df19-4210-d7df-9c069833ee81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 332.1MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 29.7MB/s \n",
            "\u001b[?25hCollecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: tf-estimator-nightly, google-pasta, tb-nightly, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ4Zpfeg6Pti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, GRU, Dropout, Embedding\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZWzQ4U-6kt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 8\n",
        "start_note = 1\n",
        "end_note = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55wRjJDdYSgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "note_to_label = {(0, 0, 0):0}\n",
        "label_to_note = {0:(0, 0, 0)}\n",
        "\n",
        "data = np.load('Data.npy')\n",
        "tokenized = []\n",
        "for song in data:\n",
        "  s = [start_note]\n",
        "  for row in song:\n",
        "    note = tuple(row)\n",
        "    if note == (0, 0, 0):\n",
        "      continue\n",
        "    if note not in note_to_label:\n",
        "      note_to_label[note] = len(note_to_label) + 2\n",
        "      label_to_note[note_to_label[note]] = note\n",
        "    s.append(note_to_label[note])\n",
        "  s.append(end_note)\n",
        "  tokenized.append(s)\n",
        "\n",
        "num_notes = len(note_to_label) + 3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHnfvcZ-aae3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = keras.preprocessing.sequence.pad_sequences(tokenized, maxlen=maxlen, padding='post')\n",
        "y = np.hstack([x[:, 1:], np.zeros((x.shape[0], 1), dtype=np.int)])\n",
        "n = x.shape[0]\n",
        "maxlen = x.shape[1]\n",
        "\n",
        "indexes = np.random.permutation(n)\n",
        "split_index = int(0.9 * n)\n",
        "x_train = x[indexes[:split_index]]\n",
        "x_val = x[indexes[split_index:]]\n",
        "y_train = y[indexes[:split_index]]\n",
        "y_val = y[indexes[split_index:]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI39M5C05aM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = keras.models.Sequential()\n",
        "# model.add(Embedding(notes_size, embedding_size))\n",
        "# model.add(GRU(16, return_sequences=True))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(notes_size, activation='softmax'))\n",
        "# model.summary()\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.embedding = Embedding(num_notes, embedding_size)\n",
        "    self.gru = GRU(16, return_sequences=True, return_state=True)\n",
        "    self.dropout = Dropout(0.2)\n",
        "    self.d = Dense(num_notes, activation='softmax')\n",
        "\n",
        "  def call(self, x, state=None):\n",
        "    x = self.embedding(x)\n",
        "    x, output_state = self.gru(x, initial_state=state)\n",
        "    x = self.dropout(x)\n",
        "    x = self.d(x)\n",
        "    return x, output_state\n",
        "\n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3xJ6e6aWnVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_metric_train = keras.metrics.Mean()\n",
        "loss_metric_val = keras.metrics.Mean()\n",
        "\n",
        "scc = keras.losses.SparseCategoricalCrossentropy()\n",
        "def loss_function(y, output):\n",
        "  return scc(y, output, sample_weight=tf.math.not_equal(y, 0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn9iXJns7NeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y, model):\n",
        "  with tf.GradientTape() as tape:    \n",
        "    output = model(x)[0]\n",
        "    loss = loss_function(y, output)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  loss_metric_train(loss)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_step(x, y, model):\n",
        "  output = model(x)[0]\n",
        "  loss = loss_function(y, output)\n",
        "  \n",
        "  loss_metric_val(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iipXewUt7udF",
        "colab_type": "code",
        "outputId": "8171f1ea-35e7-4f87-e7cf-0eb83aa3eb47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        }
      },
      "source": [
        "epochs = 200\n",
        "batch_size = 1\n",
        "\n",
        "# model.compile(optimizer='adam', loss=loss_function)\n",
        "# model.fit(x, y, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(x_val.shape[0])\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "  for x, y in train_data:\n",
        "    train_step(x, y, model)\n",
        "  for x, y in val_data:\n",
        "    val_step(x, y, model)\n",
        "  if epoch % 1 == 0:\n",
        "    end = time.time()\n",
        "    print(f'Epoch {epoch}, Time: {end - start: .3f}s, Loss: {loss_metric_train.result(): .4f}, Validation Loss: {loss_metric_val.result(): .4f}')\n",
        "    start = time.time()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Time:  4.323s, Loss:  1.4027, Validation Loss:  1.0693\n",
            "Epoch 1, Time:  2.216s, Loss:  1.2970, Validation Loss:  1.0636\n",
            "Epoch 2, Time:  2.193s, Loss:  1.2594, Validation Loss:  1.0584\n",
            "Epoch 3, Time:  2.187s, Loss:  1.2324, Validation Loss:  1.0438\n",
            "Epoch 4, Time:  2.065s, Loss:  1.2043, Validation Loss:  1.0253\n",
            "Epoch 5, Time:  2.013s, Loss:  1.1785, Validation Loss:  1.0080\n",
            "Epoch 6, Time:  1.995s, Loss:  1.1564, Validation Loss:  0.9928\n",
            "Epoch 7, Time:  1.992s, Loss:  1.1377, Validation Loss:  0.9798\n",
            "Epoch 8, Time:  2.000s, Loss:  1.1218, Validation Loss:  0.9686\n",
            "Epoch 9, Time:  1.998s, Loss:  1.1083, Validation Loss:  0.9591\n",
            "Epoch 10, Time:  2.006s, Loss:  1.0966, Validation Loss:  0.9508\n",
            "Epoch 11, Time:  1.998s, Loss:  1.0863, Validation Loss:  0.9434\n",
            "Epoch 12, Time:  2.010s, Loss:  1.0772, Validation Loss:  0.9369\n",
            "Epoch 13, Time:  2.014s, Loss:  1.0691, Validation Loss:  0.9310\n",
            "Epoch 14, Time:  2.024s, Loss:  1.0616, Validation Loss:  0.9256\n",
            "Epoch 15, Time:  2.033s, Loss:  1.0548, Validation Loss:  0.9207\n",
            "Epoch 16, Time:  2.002s, Loss:  1.0486, Validation Loss:  0.9162\n",
            "Epoch 17, Time:  2.007s, Loss:  1.0428, Validation Loss:  0.9121\n",
            "Epoch 18, Time:  2.012s, Loss:  1.0375, Validation Loss:  0.9083\n",
            "Epoch 19, Time:  2.028s, Loss:  1.0325, Validation Loss:  0.9047\n",
            "Epoch 20, Time:  2.061s, Loss:  1.0278, Validation Loss:  0.9014\n",
            "Epoch 21, Time:  2.008s, Loss:  1.0234, Validation Loss:  0.8983\n",
            "Epoch 22, Time:  2.004s, Loss:  1.0192, Validation Loss:  0.8953\n",
            "Epoch 23, Time:  1.998s, Loss:  1.0152, Validation Loss:  0.8925\n",
            "Epoch 24, Time:  2.011s, Loss:  1.0114, Validation Loss:  0.8897\n",
            "Epoch 25, Time:  2.018s, Loss:  1.0077, Validation Loss:  0.8871\n",
            "Epoch 26, Time:  2.021s, Loss:  1.0041, Validation Loss:  0.8846\n",
            "Epoch 27, Time:  2.000s, Loss:  1.0007, Validation Loss:  0.8821\n",
            "Epoch 28, Time:  2.002s, Loss:  0.9973, Validation Loss:  0.8798\n",
            "Epoch 29, Time:  2.003s, Loss:  0.9942, Validation Loss:  0.8776\n",
            "Epoch 30, Time:  1.994s, Loss:  0.9911, Validation Loss:  0.8755\n",
            "Epoch 31, Time:  2.033s, Loss:  0.9881, Validation Loss:  0.8735\n",
            "Epoch 32, Time:  2.012s, Loss:  0.9853, Validation Loss:  0.8716\n",
            "Epoch 33, Time:  1.997s, Loss:  0.9826, Validation Loss:  0.8697\n",
            "Epoch 34, Time:  2.012s, Loss:  0.9799, Validation Loss:  0.8680\n",
            "Epoch 35, Time:  1.995s, Loss:  0.9774, Validation Loss:  0.8663\n",
            "Epoch 36, Time:  2.017s, Loss:  0.9749, Validation Loss:  0.8647\n",
            "Epoch 37, Time:  2.013s, Loss:  0.9725, Validation Loss:  0.8631\n",
            "Epoch 38, Time:  2.015s, Loss:  0.9702, Validation Loss:  0.8616\n",
            "Epoch 39, Time:  2.163s, Loss:  0.9679, Validation Loss:  0.8602\n",
            "Epoch 40, Time:  2.205s, Loss:  0.9657, Validation Loss:  0.8588\n",
            "Epoch 41, Time:  2.220s, Loss:  0.9636, Validation Loss:  0.8574\n",
            "Epoch 42, Time:  2.206s, Loss:  0.9615, Validation Loss:  0.8561\n",
            "Epoch 43, Time:  2.159s, Loss:  0.9594, Validation Loss:  0.8548\n",
            "Epoch 44, Time:  1.992s, Loss:  0.9574, Validation Loss:  0.8535\n",
            "Epoch 45, Time:  2.008s, Loss:  0.9554, Validation Loss:  0.8523\n",
            "Epoch 46, Time:  1.994s, Loss:  0.9535, Validation Loss:  0.8511\n",
            "Epoch 47, Time:  2.001s, Loss:  0.9516, Validation Loss:  0.8500\n",
            "Epoch 48, Time:  2.015s, Loss:  0.9497, Validation Loss:  0.8489\n",
            "Epoch 49, Time:  2.019s, Loss:  0.9479, Validation Loss:  0.8478\n",
            "Epoch 50, Time:  2.017s, Loss:  0.9461, Validation Loss:  0.8467\n",
            "Epoch 51, Time:  2.018s, Loss:  0.9443, Validation Loss:  0.8456\n",
            "Epoch 52, Time:  2.008s, Loss:  0.9425, Validation Loss:  0.8446\n",
            "Epoch 53, Time:  2.222s, Loss:  0.9408, Validation Loss:  0.8436\n",
            "Epoch 54, Time:  2.243s, Loss:  0.9391, Validation Loss:  0.8426\n",
            "Epoch 55, Time:  2.137s, Loss:  0.9374, Validation Loss:  0.8416\n",
            "Epoch 56, Time:  2.017s, Loss:  0.9358, Validation Loss:  0.8406\n",
            "Epoch 57, Time:  2.008s, Loss:  0.9342, Validation Loss:  0.8397\n",
            "Epoch 58, Time:  2.005s, Loss:  0.9326, Validation Loss:  0.8388\n",
            "Epoch 59, Time:  1.996s, Loss:  0.9310, Validation Loss:  0.8379\n",
            "Epoch 60, Time:  2.017s, Loss:  0.9294, Validation Loss:  0.8370\n",
            "Epoch 61, Time:  2.005s, Loss:  0.9279, Validation Loss:  0.8361\n",
            "Epoch 62, Time:  2.011s, Loss:  0.9264, Validation Loss:  0.8353\n",
            "Epoch 63, Time:  2.023s, Loss:  0.9249, Validation Loss:  0.8345\n",
            "Epoch 64, Time:  2.006s, Loss:  0.9234, Validation Loss:  0.8337\n",
            "Epoch 65, Time:  2.017s, Loss:  0.9220, Validation Loss:  0.8329\n",
            "Epoch 66, Time:  2.004s, Loss:  0.9205, Validation Loss:  0.8321\n",
            "Epoch 67, Time:  2.001s, Loss:  0.9191, Validation Loss:  0.8313\n",
            "Epoch 68, Time:  2.007s, Loss:  0.9177, Validation Loss:  0.8306\n",
            "Epoch 69, Time:  2.005s, Loss:  0.9164, Validation Loss:  0.8299\n",
            "Epoch 70, Time:  2.029s, Loss:  0.9150, Validation Loss:  0.8292\n",
            "Epoch 71, Time:  2.016s, Loss:  0.9137, Validation Loss:  0.8285\n",
            "Epoch 72, Time:  2.016s, Loss:  0.9124, Validation Loss:  0.8278\n",
            "Epoch 73, Time:  2.004s, Loss:  0.9111, Validation Loss:  0.8271\n",
            "Epoch 74, Time:  2.010s, Loss:  0.9098, Validation Loss:  0.8265\n",
            "Epoch 75, Time:  2.019s, Loss:  0.9086, Validation Loss:  0.8258\n",
            "Epoch 76, Time:  2.016s, Loss:  0.9074, Validation Loss:  0.8252\n",
            "Epoch 77, Time:  2.014s, Loss:  0.9062, Validation Loss:  0.8246\n",
            "Epoch 78, Time:  2.109s, Loss:  0.9050, Validation Loss:  0.8240\n",
            "Epoch 79, Time:  2.212s, Loss:  0.9038, Validation Loss:  0.8234\n",
            "Epoch 80, Time:  2.237s, Loss:  0.9026, Validation Loss:  0.8228\n",
            "Epoch 81, Time:  2.201s, Loss:  0.9015, Validation Loss:  0.8223\n",
            "Epoch 82, Time:  2.204s, Loss:  0.9004, Validation Loss:  0.8217\n",
            "Epoch 83, Time:  2.016s, Loss:  0.8993, Validation Loss:  0.8212\n",
            "Epoch 84, Time:  2.018s, Loss:  0.8982, Validation Loss:  0.8207\n",
            "Epoch 85, Time:  2.030s, Loss:  0.8971, Validation Loss:  0.8202\n",
            "Epoch 86, Time:  2.037s, Loss:  0.8961, Validation Loss:  0.8197\n",
            "Epoch 87, Time:  2.030s, Loss:  0.8950, Validation Loss:  0.8192\n",
            "Epoch 88, Time:  2.022s, Loss:  0.8940, Validation Loss:  0.8187\n",
            "Epoch 89, Time:  2.020s, Loss:  0.8930, Validation Loss:  0.8182\n",
            "Epoch 90, Time:  2.047s, Loss:  0.8920, Validation Loss:  0.8177\n",
            "Epoch 91, Time:  2.034s, Loss:  0.8910, Validation Loss:  0.8173\n",
            "Epoch 92, Time:  2.029s, Loss:  0.8901, Validation Loss:  0.8168\n",
            "Epoch 93, Time:  2.037s, Loss:  0.8891, Validation Loss:  0.8164\n",
            "Epoch 94, Time:  2.037s, Loss:  0.8882, Validation Loss:  0.8160\n",
            "Epoch 95, Time:  2.051s, Loss:  0.8872, Validation Loss:  0.8156\n",
            "Epoch 96, Time:  2.035s, Loss:  0.8863, Validation Loss:  0.8152\n",
            "Epoch 97, Time:  2.036s, Loss:  0.8854, Validation Loss:  0.8148\n",
            "Epoch 98, Time:  2.040s, Loss:  0.8845, Validation Loss:  0.8144\n",
            "Epoch 99, Time:  2.040s, Loss:  0.8837, Validation Loss:  0.8140\n",
            "Epoch 100, Time:  2.064s, Loss:  0.8828, Validation Loss:  0.8136\n",
            "Epoch 101, Time:  2.063s, Loss:  0.8820, Validation Loss:  0.8133\n",
            "Epoch 102, Time:  2.045s, Loss:  0.8811, Validation Loss:  0.8129\n",
            "Epoch 103, Time:  2.038s, Loss:  0.8803, Validation Loss:  0.8126\n",
            "Epoch 104, Time:  2.029s, Loss:  0.8795, Validation Loss:  0.8122\n",
            "Epoch 105, Time:  2.054s, Loss:  0.8787, Validation Loss:  0.8119\n",
            "Epoch 106, Time:  2.037s, Loss:  0.8779, Validation Loss:  0.8116\n",
            "Epoch 107, Time:  2.067s, Loss:  0.8771, Validation Loss:  0.8113\n",
            "Epoch 108, Time:  2.047s, Loss:  0.8763, Validation Loss:  0.8109\n",
            "Epoch 109, Time:  2.040s, Loss:  0.8755, Validation Loss:  0.8106\n",
            "Epoch 110, Time:  2.060s, Loss:  0.8748, Validation Loss:  0.8103\n",
            "Epoch 111, Time:  2.049s, Loss:  0.8740, Validation Loss:  0.8101\n",
            "Epoch 112, Time:  2.041s, Loss:  0.8733, Validation Loss:  0.8098\n",
            "Epoch 113, Time:  2.046s, Loss:  0.8726, Validation Loss:  0.8095\n",
            "Epoch 114, Time:  2.032s, Loss:  0.8718, Validation Loss:  0.8092\n",
            "Epoch 115, Time:  2.072s, Loss:  0.8711, Validation Loss:  0.8090\n",
            "Epoch 116, Time:  2.036s, Loss:  0.8704, Validation Loss:  0.8087\n",
            "Epoch 117, Time:  2.112s, Loss:  0.8697, Validation Loss:  0.8085\n",
            "Epoch 118, Time:  2.229s, Loss:  0.8690, Validation Loss:  0.8082\n",
            "Epoch 119, Time:  2.258s, Loss:  0.8684, Validation Loss:  0.8080\n",
            "Epoch 120, Time:  2.241s, Loss:  0.8677, Validation Loss:  0.8077\n",
            "Epoch 121, Time:  2.223s, Loss:  0.8670, Validation Loss:  0.8075\n",
            "Epoch 122, Time:  2.050s, Loss:  0.8664, Validation Loss:  0.8073\n",
            "Epoch 123, Time:  2.043s, Loss:  0.8657, Validation Loss:  0.8071\n",
            "Epoch 124, Time:  2.151s, Loss:  0.8651, Validation Loss:  0.8069\n",
            "Epoch 125, Time:  2.295s, Loss:  0.8645, Validation Loss:  0.8066\n",
            "Epoch 126, Time:  2.311s, Loss:  0.8638, Validation Loss:  0.8064\n",
            "Epoch 127, Time:  2.293s, Loss:  0.8632, Validation Loss:  0.8062\n",
            "Epoch 128, Time:  2.307s, Loss:  0.8626, Validation Loss:  0.8060\n",
            "Epoch 129, Time:  2.077s, Loss:  0.8620, Validation Loss:  0.8058\n",
            "Epoch 130, Time:  2.038s, Loss:  0.8614, Validation Loss:  0.8056\n",
            "Epoch 131, Time:  2.031s, Loss:  0.8608, Validation Loss:  0.8055\n",
            "Epoch 132, Time:  2.025s, Loss:  0.8602, Validation Loss:  0.8053\n",
            "Epoch 133, Time:  2.029s, Loss:  0.8596, Validation Loss:  0.8051\n",
            "Epoch 134, Time:  2.048s, Loss:  0.8591, Validation Loss:  0.8049\n",
            "Epoch 135, Time:  2.027s, Loss:  0.8585, Validation Loss:  0.8047\n",
            "Epoch 136, Time:  2.047s, Loss:  0.8579, Validation Loss:  0.8046\n",
            "Epoch 137, Time:  2.027s, Loss:  0.8574, Validation Loss:  0.8044\n",
            "Epoch 138, Time:  2.023s, Loss:  0.8568, Validation Loss:  0.8042\n",
            "Epoch 139, Time:  2.041s, Loss:  0.8563, Validation Loss:  0.8041\n",
            "Epoch 140, Time:  2.025s, Loss:  0.8557, Validation Loss:  0.8039\n",
            "Epoch 141, Time:  2.018s, Loss:  0.8552, Validation Loss:  0.8038\n",
            "Epoch 142, Time:  2.013s, Loss:  0.8547, Validation Loss:  0.8036\n",
            "Epoch 143, Time:  2.020s, Loss:  0.8541, Validation Loss:  0.8034\n",
            "Epoch 144, Time:  2.058s, Loss:  0.8536, Validation Loss:  0.8033\n",
            "Epoch 145, Time:  2.030s, Loss:  0.8531, Validation Loss:  0.8031\n",
            "Epoch 146, Time:  2.014s, Loss:  0.8526, Validation Loss:  0.8030\n",
            "Epoch 147, Time:  2.023s, Loss:  0.8521, Validation Loss:  0.8029\n",
            "Epoch 148, Time:  2.028s, Loss:  0.8516, Validation Loss:  0.8027\n",
            "Epoch 149, Time:  2.048s, Loss:  0.8511, Validation Loss:  0.8026\n",
            "Epoch 150, Time:  2.025s, Loss:  0.8506, Validation Loss:  0.8024\n",
            "Epoch 151, Time:  2.029s, Loss:  0.8501, Validation Loss:  0.8023\n",
            "Epoch 152, Time:  2.026s, Loss:  0.8496, Validation Loss:  0.8022\n",
            "Epoch 153, Time:  2.040s, Loss:  0.8491, Validation Loss:  0.8020\n",
            "Epoch 154, Time:  2.047s, Loss:  0.8487, Validation Loss:  0.8019\n",
            "Epoch 155, Time:  2.024s, Loss:  0.8482, Validation Loss:  0.8018\n",
            "Epoch 156, Time:  2.209s, Loss:  0.8477, Validation Loss:  0.8016\n",
            "Epoch 157, Time:  2.229s, Loss:  0.8473, Validation Loss:  0.8015\n",
            "Epoch 158, Time:  2.266s, Loss:  0.8468, Validation Loss:  0.8014\n",
            "Epoch 159, Time:  2.227s, Loss:  0.8463, Validation Loss:  0.8013\n",
            "Epoch 160, Time:  2.137s, Loss:  0.8459, Validation Loss:  0.8012\n",
            "Epoch 161, Time:  2.032s, Loss:  0.8454, Validation Loss:  0.8010\n",
            "Epoch 162, Time:  2.023s, Loss:  0.8450, Validation Loss:  0.8009\n",
            "Epoch 163, Time:  2.045s, Loss:  0.8446, Validation Loss:  0.8008\n",
            "Epoch 164, Time:  2.020s, Loss:  0.8441, Validation Loss:  0.8007\n",
            "Epoch 165, Time:  2.111s, Loss:  0.8437, Validation Loss:  0.8006\n",
            "Epoch 166, Time:  2.059s, Loss:  0.8433, Validation Loss:  0.8005\n",
            "Epoch 167, Time:  2.036s, Loss:  0.8428, Validation Loss:  0.8004\n",
            "Epoch 168, Time:  2.038s, Loss:  0.8424, Validation Loss:  0.8003\n",
            "Epoch 169, Time:  2.028s, Loss:  0.8420, Validation Loss:  0.8002\n",
            "Epoch 170, Time:  2.027s, Loss:  0.8416, Validation Loss:  0.8001\n",
            "Epoch 171, Time:  2.027s, Loss:  0.8412, Validation Loss:  0.8000\n",
            "Epoch 172, Time:  2.021s, Loss:  0.8408, Validation Loss:  0.7999\n",
            "Epoch 173, Time:  2.038s, Loss:  0.8404, Validation Loss:  0.7998\n",
            "Epoch 174, Time:  2.048s, Loss:  0.8400, Validation Loss:  0.7997\n",
            "Epoch 175, Time:  2.023s, Loss:  0.8396, Validation Loss:  0.7996\n",
            "Epoch 176, Time:  2.029s, Loss:  0.8392, Validation Loss:  0.7995\n",
            "Epoch 177, Time:  2.033s, Loss:  0.8388, Validation Loss:  0.7994\n",
            "Epoch 178, Time:  2.051s, Loss:  0.8384, Validation Loss:  0.7993\n",
            "Epoch 179, Time:  2.039s, Loss:  0.8380, Validation Loss:  0.7992\n",
            "Epoch 180, Time:  2.034s, Loss:  0.8376, Validation Loss:  0.7991\n",
            "Epoch 181, Time:  2.040s, Loss:  0.8372, Validation Loss:  0.7990\n",
            "Epoch 182, Time:  2.036s, Loss:  0.8369, Validation Loss:  0.7989\n",
            "Epoch 183, Time:  2.071s, Loss:  0.8365, Validation Loss:  0.7989\n",
            "Epoch 184, Time:  2.029s, Loss:  0.8361, Validation Loss:  0.7988\n",
            "Epoch 185, Time:  2.021s, Loss:  0.8357, Validation Loss:  0.7987\n",
            "Epoch 186, Time:  2.029s, Loss:  0.8354, Validation Loss:  0.7986\n",
            "Epoch 187, Time:  2.013s, Loss:  0.8350, Validation Loss:  0.7985\n",
            "Epoch 188, Time:  2.050s, Loss:  0.8346, Validation Loss:  0.7985\n",
            "Epoch 189, Time:  2.036s, Loss:  0.8343, Validation Loss:  0.7984\n",
            "Epoch 190, Time:  2.037s, Loss:  0.8339, Validation Loss:  0.7983\n",
            "Epoch 191, Time:  2.024s, Loss:  0.8336, Validation Loss:  0.7982\n",
            "Epoch 192, Time:  2.023s, Loss:  0.8332, Validation Loss:  0.7982\n",
            "Epoch 193, Time:  2.045s, Loss:  0.8329, Validation Loss:  0.7981\n",
            "Epoch 194, Time:  2.041s, Loss:  0.8325, Validation Loss:  0.7980\n",
            "Epoch 195, Time:  2.227s, Loss:  0.8322, Validation Loss:  0.7979\n",
            "Epoch 196, Time:  2.218s, Loss:  0.8318, Validation Loss:  0.7979\n",
            "Epoch 197, Time:  2.231s, Loss:  0.8315, Validation Loss:  0.7978\n",
            "Epoch 198, Time:  2.255s, Loss:  0.8312, Validation Loss:  0.7977\n",
            "Epoch 199, Time:  2.126s, Loss:  0.8308, Validation Loss:  0.7977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYfDGewaFKw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(model, k=3):\n",
        "  result = [start_note]\n",
        "  state = None\n",
        "  while True:\n",
        "    seq = np.array([[result[-1]]])\n",
        "    output, state = model(seq, state=state)\n",
        "    note = np.random.choice(np.argpartition(output[0][0], -k)[-k:])\n",
        "    if note == 0:\n",
        "      result.append(end_note)\n",
        "    else:\n",
        "      result.append(note)\n",
        "    if result[-1] == end_note or len(result) >= maxlen:\n",
        "      break\n",
        "  return result[1:-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvevCQVIKRUc",
        "colab_type": "code",
        "outputId": "7df59962-baa3-47dc-9c15-badd8cc85c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "generated = generate(model)\n",
        "print(len(generated))\n",
        "print(generated)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "[11, 16, 11, 16, 5, 9, 8, 5, 16, 3, 3, 6, 3, 14, 22, 7, 7, 14, 11, 3, 14, 22, 14, 22, 6, 7, 14, 6, 3, 14, 3, 6, 7, 7, 7, 14, 3, 14, 6, 14, 11, 14, 6, 3, 16, 11, 3, 11, 3, 11, 14, 6, 14, 3, 5, 16, 3, 6, 14, 3, 5, 10, 8, 5, 10, 5, 10, 8, 5, 10, 8, 9, 15, 9, 8, 10, 8, 9, 8, 5, 10, 5, 16, 3, 6, 3, 3, 11, 5, 11, 14, 6, 3, 7, 14, 6, 14, 3, 6, 14, 11, 14, 3, 6, 7, 6, 7, 7, 7, 14, 3, 14, 6, 14, 22, 7, 22, 6, 7, 6, 3, 14, 22, 7, 14, 11, 14, 6, 14, 22, 6, 7, 22, 14, 3, 6, 14, 11, 14, 3, 14, 22, 14, 22, 14, 6, 7, 6, 3, 16, 11, 3, 11, 14, 22, 14, 3, 5, 9, 8, 5, 10, 9, 15, 19, 9, 10, 8, 10, 5, 11, 14, 11, 3, 6, 3, 7, 6, 18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9riCnFapueO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_data = [generate(model) for i in range(10)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX6x-EQ-pfnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data = keras.preprocessing.sequence.pad_sequences(generated_data, maxlen=maxlen, padding='post')\n",
        "new_songs = np.array([[label_to_note[label] for label in row] for row in new_data])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgue9EAYyNkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('new_songs', new_songs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}